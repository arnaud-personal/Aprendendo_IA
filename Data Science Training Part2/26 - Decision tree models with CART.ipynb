{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4fb436d1-54e4-40d8-a130-c9548d926a38",
   "metadata": {},
   "source": [
    "<h1>Decision tree models with CART</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "257c1862-f257-4db2-8809-0658c18adf6a",
   "metadata": {},
   "source": [
    "Tarefas de classificação e regressão.<br>\n",
    "Não supervisionado para dados não lineares\n",
    "\n",
    "<h3>Objetivo</h3>\n",
    "<ul>\n",
    "    <li>Prever um valor de saída (classe ou valor contínuo) com base em decisões sucessivas que particionam os dados <br>\n",
    "        com base em suas características (features), criando uma estrutura em forma de árvore.</li>\n",
    "    <li>Em classificação, a árvore aprende a dividir os dados em categorias.</li>\n",
    "    <li>Em regressão, ela busca prever valores numéricos contínuos.</li>\n",
    "</ul>\n",
    "\n",
    "<h3>Pré-requisitos</h3>\n",
    "<ul>\n",
    "    <li>Dados tabulares bem preparados</li>\n",
    "    <li>Dados numéricos ou categóricos (os categóricos devem ser codificados, por exemplo, com OneHot ou LabelEncoder).</li>\n",
    "    <li>Sem necessidade de normalização: Diferente do KNN ou SVM, a Decision Tree não exige escala padronizada.</li>\n",
    "    <li>Rótulos para supervised learning: O algoritmo precisa de um conjunto de treino com X (atributos) e y (classe ou valor).</li>\n",
    "    <li>Evitar overfitting: Árvores muito profundas podem memorizar os dados (overfit).É comum limitar a profundidade, <br>\n",
    "        número mínimo de amostras por nó, etc.</li>\n",
    "</ul>\n",
    "\n",
    "<h3></h3>\n",
    "<ul>\n",
    "    <li>Vantagens</li>\n",
    "    <li>Interpretação simples (pode ser visualizada).</li>\n",
    "    <li>Suporta dados categóricos e numéricos.</li>\n",
    "    <li>Nenhuma suposição sobre a distribuição dos dados.</li>\n",
    "</ul>\n",
    "\n",
    "<h3>Desvantagens</h3>\n",
    "<ul>\n",
    "    <li>Pode overfitar se não for podada ou limitada.</li>\n",
    "    <li>Instável a pequenas variações nos dados </li>\n",
    "    <li>(isso motivou o uso de métodos como Random Forest e Gradient Boosting).</li>\n",
    "</ul>\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f339e694-a30e-4f87-89f8-81bd6f7f36bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
